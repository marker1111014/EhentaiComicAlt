import logging
import os
import requests
import uuid
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, MessageHandler, filters, CommandHandler, ContextTypes
import urllib.parse
import random
import re
import json
import time
import traceback

# --- ç·©å­˜å·²è™•ç†çš„è¨Šæ¯é€£çµï¼Œç”¨æ–¼è™•ç†é‡è¤‡è¨Šæ¯ ---
# æ ¼å¼: {(chat_id, message_id): "processed_link_string"}
# å„²å­˜æ¯å€‹è¨Šæ¯è™•ç†éçš„é€£çµï¼Œä»¥ä¾¿åˆ¤æ–·æ˜¯å¦å·²è™•ç†éç›¸åŒçš„è¨Šæ¯
PROCESSED_LINKS_CACHE = {} 

################# çµ„æ…‹å€ #################
# IMPORTANT: è«‹æ›¿æ›ç‚ºæ‚¨è‡ªå·±çš„ Telegram Bot Token
# å¦‚æœæœ‰å…©å€‹ Bot Tokenï¼Œå¯ä»¥é¸æ“‡ä¸€å€‹ä½œç‚ºä¸» Botï¼Œæˆ–è€…ä½¿ç”¨ç’°å¢ƒè®Šæ•¸
TELEGRAM_BOT_TOKEN = 'Token' 

# --- E-Hentai ç›¸é—œè¨­å®š ---
E_HEN_TAI_BASE_URL = "https://e-hentai.org/"

# --- ExHentai ç›¸é—œè¨­å®š ---
EX_HEN_TAI_BASE_URL = "https://exhentai.org/"
# IMPORTANT: è«‹æ›¿æ›æˆä½ çš„ ExHentai Cookies
EX_HENTAI_COOKIES = {
    "igneous": "COOKIES",
    "ipb_member_id": "COOKIES",
    "ipb_pass_hash": "COOKIES",
    "s": "COOKIES", # s å€¼é€šå¸¸æœƒè®Šï¼Œå»ºè­°å®šæœŸæ›´æ–°
    "sl": "COOKIES"
}

# --- Nhentai ç›¸é—œè¨­å®š ---
NHENTAI_BASE_URL = "https://nhentai.net/"

# --- ç´³å£«æ¼«ç•« (wnacg.com) ç›¸é—œè¨­å®š ---
WNACG_BASE_URL = "https://www.wnacg.com/"


# è¨­å®šæ—¥èªŒè¨˜éŒ„
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# å°‡æ­£è¦è¡¨é”å¼å®šç¾©ç‚ºå…¨åŸŸè®Šé‡ï¼Œä»¥ä¾¿åœ¨è™•ç†å‡½æ•¸ä¸­é‡è¤‡ä½¿ç”¨
e_ex_hentai_regex = re.compile(
    r'https?://(?:e-hentai|exhentai)\.org/g/\d+/[0-9a-fA-F]+/?', re.IGNORECASE
)
# Adjusted to only match x.com or twitter.com, not vxtwitter.com
# é€™å€‹æ­£å‰‡è¡¨é”å¼åœ¨åŒ¹é…æ™‚æ²’æœ‰å•é¡Œï¼Œå®ƒä¸æœƒåŒ¹é… vxtwitter.com
x_link_regex = re.compile(
    r'https?://(?:x|twitter)\.com/\S+', re.IGNORECASE
)


async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """è™•ç† /start å‘½ä»¤ã€‚"""
    await update.message.reply_text('ä½ å¥½ï¼è«‹ç™¼é€ E-Hentai/ExHentai é€£çµï¼Œæˆ‘æœƒç‚ºæ‚¨å°‹æ‰¾ç›¸é—œçš„ Nhentai å’Œç´³å£«æ¼«ç•«ä½œå“ã€‚æ‚¨ä¹Ÿå¯ä»¥ç™¼é€ X.com (Twitter) é€£çµï¼Œæˆ‘æœƒå°‡å…¶è½‰æ›ç‚º VxTwitter é€£çµã€‚')

def clean_tag(tag: str) -> str:
    """æ¸…ç†æ¨™ç±¤å­—ä¸²ï¼Œç§»é™¤å¸¸è¦‹çš„å‰ç¶´å’Œä¸å¿…è¦çš„ç©ºæ ¼ã€‚"""
    tag = tag.lower().strip()
    # ç§»é™¤ E-Hentai/ExHentai ä¸­å¸¸è¦‹çš„æ¨™ç±¤é¡å‹å‰ç¶´
    prefixes = ['artist:', 'group:', 'language:', 'parody:', 'character:', 'tag:']
    for prefix in prefixes:
        if tag.startswith(prefix):
            tag = tag[len(prefix):].strip()
    return tag

def clean_title_string(title_str: str) -> str:
    """æ¸…ç†æ¨™é¡Œå­—ä¸²ï¼Œç§»é™¤æ‰€æœ‰æ–¹æ‹¬è™Ÿ [] åŠå…¶å…§éƒ¨å…§å®¹ï¼Œ
    ä»¥åŠå…¶ä»–å¸¸è¦‹çš„æ¨™ç±¤å’Œä¸å¿…è¦çš„ç©ºæ ¼ã€‚
    æ­¤å‡½æ•¸ä¸»è¦ç”¨æ–¼ç²å–ä¹¾æ·¨çš„ã€Œä¸»æ¨™é¡Œã€å’Œã€Œå‰¯æ¨™é¡Œã€ã€‚
    """
    cleaned = title_str
    # ç§»é™¤æ‰€æœ‰æ–¹æ‹¬è™ŸåŠå…¶å…§éƒ¨å…§å®¹
    cleaned = re.sub(r'\[.*?\]', '', cleaned).strip()
    
    # ç§»é™¤åœ“æ‹¬è™Ÿå…§çš„å¸¸è¦‹æ¨™ç±¤ (å¦‚èªè¨€ã€ç‰ˆæœ¬ç­‰ï¼Œéç³»åˆ—åç¨±)
    # é€™è£¡éœ€è¦æ›´ç²¾ç¢ºï¼Œé¿å…ç§»é™¤çœŸæ­£çš„ç³»åˆ—å
    keywords_to_remove_from_paren = [
        r'\((?:Chinese|English|Japanese|æ¼¢åŒ–|ä¸­åœ‹ç¿»è¨³|æ—¥èª|è‹±è¨³|ç„¡ä¿®æ­£|DLç‰ˆ|Digital|MTL|machine translation)\)',
        r'\(C\d+\)', # Comiketæ¨™è­˜
        r'\(vol\.?\s*\d+\)', # å·/ç« ç¯€æ¨™è­˜
        r'\(v\.?\s*\d+\)' # ç‰ˆæœ¬è™Ÿ
    ]
    for kw_regex in keywords_to_remove_from_paren:
        cleaned = re.sub(kw_regex, '', cleaned, flags=re.IGNORECASE).strip()

    # ç§»é™¤å¯èƒ½å‰©ä¸‹çš„å¤šé¤˜ç©ºæ ¼æˆ–é€£å­—å…ƒ
    cleaned = re.sub(r'\s{2,}', ' ', cleaned).strip() # å¤šå€‹ç©ºæ ¼è®Šç‚ºä¸€å€‹
    cleaned = re.sub(r'^\s*-\s*', '', cleaned).strip() # é–‹é ­çš„é€£å­—å…ƒ
    cleaned = re.sub(r'\s*-\s*$', '', cleaned).strip() # çµå°¾çš„é€£å­—å…ƒ
    return cleaned

def clean_title_for_nhentai_search(title_str: str) -> str:
    """
    ç‚º Nhentai æœå°‹ç‰¹åˆ¥è¨­è¨ˆçš„æ¨™é¡Œæ¸…ç†å‡½æ•¸ã€‚
    æ¯” `clean_title_string` æ›´æº«å’Œï¼Œæœƒä¿ç•™æ–¹æ‹¬è™Ÿå’Œå¤§éƒ¨åˆ†åœ“æ‹¬è™Ÿå…§å®¹ï¼Œ
    åªç§»é™¤æœ€å¯èƒ½å¹¹æ“¾ N ç«™æœå°‹çš„ç‰¹å®šæ¨™è­˜ã€‚
    """
    cleaned = title_str
    # ç§»é™¤åœ“æ‹¬è™Ÿå…§çš„èªè¨€ã€ç¿»è­¯æ¨™è­˜ã€å·è™Ÿã€ç‰ˆæœ¬è™Ÿç­‰ï¼Œä½†ä¿ç•™å…¶ä»–æ‹¬è™Ÿå…§å®¹
    keywords_to_remove_from_paren_nhentai = [
        r'\((?:Chinese|English|Japanese|æ¼¢åŒ–|ä¸­åœ‹ç¿»è¨³|æ—¥èª|è‹±è¨³|ç„¡ä¿®æ­£|DLç‰ˆ|Digital|MTL|machine translation|color|å½©è‰²)\)',
        r'\(C\d+\)', # Comiketæ¨™è­˜
        r'\(vol\.?\s*\d+\)', # å·/ç« ç¯€æ¨™è­˜
        r'\(v\.?\s*\d+\)' # ç‰ˆæœ¬è™Ÿ
    ]
    for kw_regex in keywords_to_remove_from_paren_nhentai:
        cleaned = re.sub(kw_regex, '', cleaned, flags=re.IGNORECASE).strip()

    # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    cleaned = re.sub(r'\s{2,}', ' ', cleaned).strip()
    
    # ç§»é™¤é–‹é ­æˆ–çµå°¾çš„é€£å­—å…ƒ
    cleaned = re.sub(r'^\s*-\s*', '', cleaned).strip() 
    cleaned = re.sub(r'\s*-\s*$', '', cleaned).strip()

    return cleaned


def calculate_similarity(e_ex_hentai_tags: dict, nhentai_tags: dict) -> float:
    """
    è¨ˆç®— E-Hentai/ExHentai æ¨™ç±¤èˆ‡ Nhentai æ¨™ç±¤çš„ç›¸ä¼¼åº¦ã€‚
    æ¨™ç±¤é¡å‹æœƒå½±éŸ¿æ¬Šé‡ï¼Œå®Œå…¨åŒ¹é…çš„æ¨™ç±¤æ¬Šé‡æ›´é«˜ã€‚
    """
    if not e_ex_hentai_tags or not nhentai_tags:
        return 0.0

    total_possible_score = 0
    actual_score = 0

    # å®šç¾©æ¨™ç±¤é¡å‹æ¬Šé‡
    tag_weights = {
        'artist': 5,
        'group': 4,
        'parody': 3,
        'character': 3,
        'language': 2,
        'tag': 1
    }

    # è™•ç† E-Hentai/ExHentai æ¨™ç±¤
    e_ex_hentai_processed_tags = {}
    for tag_type, tags in e_ex_hentai_tags.items():
        cleaned_tags = [clean_tag(tag) for tag in tags]
        e_ex_hentai_processed_tags[tag_type] = cleaned_tags
        total_possible_score += len(cleaned_tags) * tag_weights.get(tag_type, 1)

    # è™•ç† Nhentai æ¨™ç±¤
    nhentai_processed_tags = {}
    for tag_obj in nhentai_tags:
        tag_type = tag_obj.get('type')
        tag_name = clean_tag(tag_obj.get('name'))
        if tag_type and tag_name:
            nhentai_processed_tags.setdefault(tag_type, []).append(tag_name)
        
    # è¨ˆç®—åˆ†æ•¸
    for e_type, e_tags in e_ex_hentai_processed_tags.items():
        weight = tag_weights.get(e_type, 1)
        if e_type in nhentai_processed_tags:
            for e_tag in e_tags:
                if e_tag in nhentai_processed_tags[e_type]:
                    actual_score += weight # åŒé¡å‹æ¨™ç±¤åŒ¹é…
                # else:
                #     # è€ƒæ…®è·¨é¡å‹ä½†æ–‡å­—åŒ¹é…çš„æ¨™ç±¤ (ä¾‹å¦‚ E-Hentai çš„ character åœ¨ Nhentai æ˜¯ tag)
                #     # é€™éƒ¨åˆ†æ¯”è¼ƒè¤‡é›œä¸”å¯èƒ½å¼•å…¥é›œè¨Šï¼Œæš«ä¸å¯¦ä½œ
    
    if total_possible_score == 0:
        return 0.0
        
    return (actual_score / total_possible_score) * 100

def get_e_ex_hentai_info(gallery_url: str) -> tuple[str, dict, str, str, str]:
    """
    å¾ E-Hentai/ExHentai é€£çµæŠ“å–ç•«å»Šæ¨™é¡Œå’Œæ¨™ç±¤ã€‚
    å˜—è©¦è§£æä¸¦è¿”å›ä¸»æ¨™é¡Œã€å‰¯æ¨™é¡Œï¼Œä»¥åŠé¡å¤–çš„æ—¥æ–‡æ¨™é¡Œï¼ˆå¦‚æœå­˜åœ¨ä¸”ä¸åŒï¼‰ã€‚
    è¿”å›: (ç¸½é«”æ¨™é¡Œ, æ¨™ç±¤å­—å…¸, ä¸»æ¨™é¡Œ, å‰¯æ¨™é¡Œ, é¡å¤–æ—¥æ–‡ä¸»æ¨™é¡Œ)
    """
    cookies_to_use = EX_HENTAI_COOKIES if "exhentai" in gallery_url else {}
    
    try:
        response = requests.get(gallery_url, cookies=cookies_to_use, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # ç²å–è‹±æ–‡æ¨™é¡Œ (gn)
        english_title_element = soup.find(id='gn')
        full_english_title = english_title_element.get_text(strip=True) if english_title_element else ""

        # ç²å–æ—¥æ–‡æ¨™é¡Œ (gj)
        japanese_title_element = soup.find(id='gj')
        full_japanese_title = japanese_title_element.get_text(strip=True) if japanese_title_element else ""
        
        # é è¨­ç¸½é«”æ¨™é¡Œç‚ºè‹±æ–‡æ¨™é¡Œï¼Œå¦‚æœè‹±æ–‡æ¨™é¡Œä¸å­˜åœ¨å‰‡ä½¿ç”¨æ—¥æ–‡æ¨™é¡Œ
        full_gallery_title = full_english_title if full_english_title else full_japanese_title
        if not full_gallery_title:
            full_gallery_title = "æœªçŸ¥æ¨™é¡Œ"

        # --- è§£æä¸»æ¨™é¡Œå’Œå‰¯æ¨™é¡Œ ---
        main_title_parsed = ""
        subtitle_parsed = ""
        extra_japanese_main_title = "" # ç”¨æ–¼å„²å­˜æ¸…ç†å¾Œçš„æ—¥æ–‡ä¸»æ¨™é¡Œ

        # å„ªå…ˆè™•ç†è‹±æ–‡æ¨™é¡Œä¾†ç¢ºå®š main_title_parsed å’Œ subtitle_parsed
        current_processing_title = full_english_title if full_english_title else full_gallery_title

        # 1. å˜—è©¦ä»¥ '-' åˆ†å‰²ä¸»å‰¯æ¨™é¡Œ (åœ¨æ¸…ç†æ–¹æ‹¬è™Ÿä¹‹å¾Œé€²è¡Œ)
        cleaned_current_processing_title = clean_title_string(current_processing_title)
        
        if ' - ' in cleaned_current_processing_title:
            parts = [p.strip() for p in cleaned_current_processing_title.split(' - ', 1)]
            if len(parts) == 2:
                main_title_parsed = parts[0]
                subtitle_parsed = parts[1]
        
        # å¦‚æœæ²’æœ‰å¾ '-' åˆ†å‰²å‡ºå‰¯æ¨™é¡Œï¼Œæˆ–å‰¯æ¨™é¡Œä¸å¤ å¥½ï¼Œå‰‡å˜—è©¦å¾åœ“æ‹¬è™Ÿä¸­æå–
        # æ³¨æ„ï¼šæ­¤è™•çš„åœ“æ‹¬è™Ÿå…§å…§å®¹ï¼Œå¦‚æœä¸æ˜¯å¸¸è¦‹çš„æ¨™ç±¤ï¼ˆå¦‚èªè¨€ã€ç‰ˆæœ¬ï¼‰ï¼Œå‰‡è¦–ç‚ºå‰¯æ¨™é¡Œ
        if not subtitle_parsed or len(subtitle_parsed) < 3: # å†æ¬¡å˜—è©¦ï¼Œæˆ–è¦†è“‹çŸ­çš„å‰¯æ¨™é¡Œ
            temp_title_for_paren_check = cleaned_current_processing_title # åœ¨æ­¤åŸºç¤ä¸Šæª¢æŸ¥åœ“æ‹¬è™Ÿ
            match_series = re.search(r'\(([^)]+)\)$', temp_title_for_paren_check)
            if match_series:
                potential_subtitle_from_paren = match_series.group(1).strip()
                # åˆ¤æ–·æ˜¯å¦ç‚ºæœ‰æ•ˆå‰¯æ¨™é¡Œï¼ˆä¸æ˜¯å–®ç´”çš„æ•¸å­—å¹´ä»½ã€å·è™Ÿã€ç‰ˆæœ¬è™Ÿæˆ–èªè¨€ï¼‰
                if len(potential_subtitle_from_paren) > 2 and \
                   not re.fullmatch(r'\d{4}', potential_subtitle_from_paren) and \
                   not re.fullmatch(r'(?:vol|ch|chapter)\.?\s*\d+', potential_subtitle_from_paren, flags=re.IGNORECASE) and \
                   not re.fullmatch(r'v\.?\s*\d+', potential_subtitle_from_paren, flags=re.IGNORECASE) and \
                   potential_subtitle_from_paren.lower() not in ['chinese', 'english', 'japanese', 'digital', 'æ¼¢åŒ–', 'dlç‰ˆ', 'ç„¡ä¿®æ­£', 'korean', 'thai', 'vietnamese']:
                    
                    subtitle_parsed = potential_subtitle_from_paren
                    main_title_parsed = temp_title_for_paren_check[:match_series.start()].strip()
                else:
                    main_title_parsed = temp_title_for_paren_check # ä½¿ç”¨æ¸…ç†éä½†æœªç§»é™¤æ‹¬è™Ÿçš„ä¸»æ¨™é¡Œ

        # å¦‚æœä¸»æ¨™é¡Œä»ç„¶æ˜¯ç©ºæˆ–æ²’æœ‰å¾ä¸Šè¿°æ–¹æ³•ä¸­ç²å¾—ï¼Œå‰‡ä½¿ç”¨åŸå§‹è‹±æ–‡æ¨™é¡Œä½œç‚ºåŸºç¤
        if not main_title_parsed:
            main_title_parsed = clean_title_string(full_english_title or full_japanese_title or full_gallery_title)
            subtitle_parsed = "" # æ¸…ç©ºå‰¯æ¨™é¡Œï¼Œå› ç‚ºå¯èƒ½éŒ¯èª¤æ‹†åˆ†

        # å¦‚æœå‰¯æ¨™é¡Œå’Œä¸»æ¨™é¡Œç›¸åŒæˆ–æ¥µå…¶ç›¸ä¼¼ï¼Œå‰‡æ¸…ç©ºå‰¯æ¨™é¡Œ
        if subtitle_parsed and (main_title_parsed.lower() == subtitle_parsed.lower() or 
                                main_title_parsed.lower().startswith(subtitle_parsed.lower())):
            subtitle_parsed = ""
        
        # å¦‚æœ extra_japanese_main_title å’Œ main_title_parsed ç›¸åŒï¼ˆå¯èƒ½æ˜¯å› ç‚ºåªæœ‰è‹±æ–‡æ¨™é¡Œæˆ–å…©è€…å®Œå…¨ä¸€æ¨£ï¼‰
        # é€™è£¡æ‡‰è©²è™•ç†æ—¥æ–‡æ¨™é¡Œçš„æ¸…ç†ï¼Œä¸¦åœ¨ä¸èˆ‡è‹±æ–‡æ¨™é¡Œé‡è¤‡æ™‚ä½¿ç”¨
        if full_japanese_title and full_japanese_title != full_english_title:
             cleaned_japanese_title = clean_title_string(full_japanese_title)
             if cleaned_japanese_title and cleaned_japanese_title != main_title_parsed:
                 extra_japanese_main_title = cleaned_japanese_title

        tags = {}
        taglist_div = soup.find(id='taglist')
        if taglist_div:
            for row in taglist_div.find_all('tr'):
                tc_td = row.find('td', class_='tc')
                if tc_td:
                    raw_type = tc_td.text.replace(':', '').strip().lower()
                    type_mapping = {
                        'artist': 'artist', 'group': 'group', 'parody': 'parody', 
                        'character': 'character', 'language': 'language', 'female': 'tag', 
                        'male': 'tag', 'fetish': 'tag', 'misc': 'tag', 'reclass': 'tag', 'other': 'tag' # åŠ å…¥ 'other'
                    }
                    mapped_type = type_mapping.get(raw_type, 'tag')
                    
                    gt_divs = row.find_all('div', class_=re.compile(r'gt[a-z]'))
                    for div_tag in gt_divs:
                        a_tag = div_tag.find('a')
                        if a_tag:
                            tag_name = a_tag.text.strip()
                            if tag_name:
                                tags.setdefault(mapped_type, []).append(tag_name)

        logger.info(f"E/ExHentai æ¨™é¡Œè§£æçµæœ: ç¸½æ¨™é¡Œ='{full_gallery_title}', ä¸»æ¨™é¡Œ='{main_title_parsed}', å‰¯æ¨™é¡Œ='{subtitle_parsed}', é¡å¤–æ—¥æ–‡ä¸»æ¨™é¡Œ='{extra_japanese_main_title}'")
        return full_gallery_title, tags, main_title_parsed, subtitle_parsed, extra_japanese_main_title

    except requests.exceptions.RequestException as e:
        logger.error(f"å¾ {gallery_url} ç²å–è³‡è¨Šå¤±æ•—: {e}")
        return "ç²å–å¤±æ•—", {}, "", "", ""
    except Exception as e:
        logger.error(f"è§£æ {gallery_url} é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
        return "è§£æéŒ¯èª¤", {}, "", "", ""

def search_nhentai(full_query: str, main_title: str, subtitle: str, extra_japanese_main_title: str, original_tags: dict) -> list:
    """
    åœ¨ Nhentai ä¸Šæœå°‹ç›¸é—œä½œå“ï¼Œä¸¦è¨ˆç®—ç›¸ä¼¼åº¦ã€‚
    æœƒå˜—è©¦å¤šç¨®æŸ¥è©¢æ–¹å¼ä»¥æé«˜å‘½ä¸­ç‡ã€‚
    """
    search_results = []
    
    queries_to_try = []

    # 1. å„ªå…ˆåŠ å…¥åŸå§‹çš„ E-Hentai/ExHentai å®Œæ•´æ¨™é¡Œ (full_query)
    # Nhentai æœå°‹å¯èƒ½èƒ½è™•ç†æ–¹æ‹¬è™Ÿç­‰ç‰¹æ®Šå­—å…ƒ
    if full_query:
        queries_to_try.append(full_query)

    # 2. åŠ å…¥ç‚º Nhentai æœå°‹å°ˆé–€æ¸…ç†éçš„æ¨™é¡Œ
    nhentai_cleaned_main_title = clean_title_for_nhentai_search(main_title)
    if nhentai_cleaned_main_title and nhentai_cleaned_main_title not in queries_to_try:
        queries_to_try.append(nhentai_cleaned_main_title)
    
    # 3. åŠ å…¥ç‚º Nhentai æœå°‹å°ˆé–€æ¸…ç†éçš„æ—¥æ–‡æ¨™é¡Œ (å¦‚æœå­˜åœ¨ä¸”ä¸åŒ)
    if extra_japanese_main_title:
        nhentai_cleaned_japanese_title = clean_title_for_nhentai_search(extra_japanese_main_title)
        if nhentai_cleaned_japanese_title and nhentai_cleaned_japanese_title not in queries_to_try and nhentai_cleaned_japanese_title != nhentai_cleaned_main_title:
            queries_to_try.append(nhentai_cleaned_japanese_title)

    # 4. å¾åŸå§‹æ¨™ç±¤ä¸­æå–ä½œè€…å’Œçµ„åˆ¥ä½œç‚ºé¡å¤–æŸ¥è©¢é—œéµå­—
    artists = original_tags.get('artist', [])
    groups = original_tags.get('group', [])
    
    # çµ„åˆæŸ¥è©¢è©ï¼š"ä½œè€…" + æ¸…ç†å¾Œçš„æ¨™é¡Œï¼Œæˆ–å–®ç¨çš„"ä½œè€…"
    for artist in artists:
        if nhentai_cleaned_main_title:
            queries_to_try.append(f'"{artist}" {nhentai_cleaned_main_title}')
        queries_to_try.append(f'"{artist}"') 
    for group in groups:
        if nhentai_cleaned_main_title:
            queries_to_try.append(f'"{group}" {nhentai_cleaned_main_title}')
        queries_to_try.append(f'"{group}"')

    # å»é‡
    queries_to_try = list(dict.fromkeys(queries_to_try))
    queries_to_try = [q for q in queries_to_try if q] # ç¢ºä¿æŸ¥è©¢è©éç©º
    logger.info(f"Nhentai æœå°‹å˜—è©¦çš„æŸ¥è©¢è©: {queries_to_try}")

    for q in queries_to_try:
        if not q: continue
        search_url = f"{NHENTAI_BASE_URL}api/galleries/search?query={urllib.parse.quote(q)}"
        logger.info(f"Nhentai æœå°‹ URL: {search_url}")
        try:
            response = requests.get(search_url, timeout=10)
            response.raise_for_status()
            data = response.json()

            if data and data.get('result'):
                for entry in data['result']:
                    nhentai_id = entry['id']
                    # å„ªå…ˆä½¿ç”¨ pretty æ¨™é¡Œï¼Œå…¶æ¬¡æ˜¯è‹±æ–‡æ¨™é¡Œ
                    nhentai_title_obj = entry['title'].get('pretty') or entry['title'].get('english')
                    nhentai_tags = entry['tags']
                    nhentai_url = f"{NHENTAI_BASE_URL}g/{nhentai_id}/"

                    similarity_score = calculate_similarity(original_tags, nhentai_tags)
                    search_results.append({
                        'title': nhentai_title_obj,
                        'url': nhentai_url,
                        'similarity': similarity_score
                    })
                # å¦‚æœæ‰¾åˆ°ä»»ä½•çµæœï¼Œå°±åœæ­¢æ›´å¤šå˜—è©¦ï¼Œé¿å…é‡è¤‡å’Œä¸ç›¸é—œçµæœ
                if search_results:
                    break 

        except requests.exceptions.RequestException as e:
            logger.warning(f"Nhentai æœå°‹ '{q}' å¤±æ•—: {e}")
        except json.JSONDecodeError:
            logger.warning(f"Nhentai æœå°‹ '{q}' è¿”å›é JSON éŸ¿æ‡‰ã€‚")
        except Exception as e:
            logger.error(f"Nhentai æœå°‹ '{q}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    search_results = sorted(search_results, key=lambda x: x['similarity'], reverse=True)
    return search_results[:5] # è¿”å›å‰5å€‹æœ€ç›¸é—œçš„çµæœ

def get_wnacg_info(gallery_url: str) -> dict:
    """
    å¾ç´³å£«æ¼«ç•«ç•«å»Šé é¢ç²å–é¡å¤–è³‡è¨Šï¼Œä¾‹å¦‚æ˜¯å¦ç‚ºæ¼¢åŒ–ç‰ˆã€‚
    """
    try:
        response = requests.get(gallery_url, timeout=5)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        is_translated = False

        # --- æœ€å¯é çš„åˆ¤æ–·æ–¹å¼1ï¼šæ˜ç¢ºçš„åˆ†é¡æ¨™ç±¤ (e.g., åˆ†é¡ï¼šåŒäººèªŒï¼æ¼¢åŒ–) ---
        category_label = soup.find('label', string=re.compile(r'åˆ†é¡ï¼š.*?æ¼¢åŒ–'))
        if category_label:
            is_translated = True
            logger.info(f"Found 'æ¼¢åŒ–' in explicit category label for {gallery_url}")
            return {"is_translated": is_translated} 
        
        # --- æœ€å¯é çš„åˆ¤æ–·æ–¹å¼2ï¼šéºµåŒ…å±‘å°èˆªä¸­çš„æ¼¢åŒ–åˆ†é¡é€£çµ ---
        # æª¢æŸ¥é€£çµæ˜¯å¦æŒ‡å‘ç´³å£«æ¼«ç•«çš„ç‰¹å®šæ¼¢åŒ–åˆ†é¡ ID
        # ç´³å£«æ¼«ç•«æ¼¢åŒ–åˆ†é¡çš„ href åŒ…å« 'cate-1.html' (åŒäººèªŒ), 'cate-9.html' (å–®è¡Œæœ¬), 'cate-10.html' (é›œèªŒ&çŸ­ç¯‡), 'cate-20.html' (éŸ“æ¼«)
        breadcrumb_div = soup.find('div', class_='png bread')
        if breadcrumb_div:
            for a_tag in breadcrumb_div.find_all('a'):
                href = a_tag.get('href')
                if href and ('cate-1.html' in href or 'cate-9.html' in href or 'cate-10.html' in href or 'cate-20.html' in href) \
                   and 'æ¼¢åŒ–' in a_tag.text: # ç¢ºä¿é€£çµæ–‡å­—ä¹ŸåŒ…å«'æ¼¢åŒ–'
                    is_translated = True
                    logger.info(f"Found 'æ¼¢åŒ–' category link in breadcrumb for {gallery_url}")
                    return {"is_translated": is_translated}

        # --- è¼”åŠ©åˆ¤æ–·æ–¹å¼ï¼šh2 æ¨™é¡Œä¸­æ˜¯å¦åŒ…å«ã€Œæ¼¢åŒ–çµ„ã€æˆ–ã€Œchinese translatedã€ ---
        # é€™å€‹å…ƒç´ æ˜¯é‡å°ç‰¹å®šæ¼«ç•«çš„æ¨™é¡Œï¼Œå¯èƒ½åŒ…å«æ¼¢åŒ–çµ„å
        title_element_h2 = soup.find('h2') # æ‰¾åˆ° h2 æ¨™ç±¤ï¼Œä¸é™ class
        if title_element_h2 and ('æ¼¢åŒ–çµ„' in title_element_h2.text or 'chinese translated' in title_element_h2.text.lower()):
            is_translated = True
            logger.info(f"Found 'æ¼¢åŒ–çµ„' in h2 title for {gallery_url}")
            return {"is_translated": is_translated} # å¦‚æœæ¨™é¡Œæ˜ç¢ºï¼Œç›´æ¥è¿”å›

        # --- è¼”åŠ©åˆ¤æ–·æ–¹å¼ï¼šæ¨™ç±¤ä¸­æ˜¯å¦åŒ…å« 'æ¼¢åŒ–' æˆ– 'chinese' ---
        # é€™æ˜¯è¼ƒå¼±çš„æª¢æŸ¥ï¼Œå› ç‚ºæ¨™ç±¤å¯èƒ½ä¸å¤ ç²¾ç¢º
        tag_container = soup.find('div', class_='addtags') 
        if tag_container:
            for a_tag in tag_container.find_all('a', class_='tagshow'): # åªæª¢æŸ¥ class ç‚º 'tagshow' çš„æ¨™ç±¤
                tag_name = a_tag.text.strip().lower()
                if 'æ¼¢åŒ–' in tag_name or 'chinese' in tag_name:
                    is_translated = True
                    logger.info(f"Found 'æ¼¢åŒ–' in tag for {gallery_url}")
                    break # æ‰¾åˆ°ä¸€å€‹å°±å¤ äº†

        # æ³¨æ„ï¼šå·²ç§»é™¤å° <title> æ¨™ç±¤å’Œ <meta name="description"> çš„æª¢æŸ¥ï¼Œ
        # å› ç‚ºå®ƒå€‘åŒ…å«ç¶²ç«™é€šç”¨è³‡è¨Šï¼Œå®¹æ˜“å°è‡´èª¤åˆ¤ã€‚

        return {"is_translated": is_translated}

    except requests.exceptions.RequestException as e:
        logger.warning(f"å¾ç´³å£«æ¼«ç•« {gallery_url} ç²å–è©³ç´°è³‡è¨Šå¤±æ•—: {e}")
        return {"is_translated": False}
    except Exception as e:
        logger.error(f"è§£æç´³å£«æ¼«ç•« {gallery_url} é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
        return {"is_translated": False}


def search_wnacg_by_title(full_query: str, main_title: str = "", subtitle: str = "", extra_japanese_main_title: str = "") -> list:
    """
    åœ¨ç´³å£«æ¼«ç•« (wnacg.com) ä¸Šæ ¹æ“šæ¨™é¡Œæœå°‹ä½œå“ã€‚
    æ¥æ”¶ä¸»æ¨™é¡Œå’Œå‰¯æ¨™é¡Œï¼Œä»¥åŠé¡å¤–çš„æ—¥æ–‡ä¸»æ¨™é¡Œï¼Œä¸¦å°‡å®ƒå€‘åŠ å…¥åˆ°æœå°‹æŸ¥è©¢è©çš„åˆ—è¡¨ä¸­ã€‚
    æ¯å€‹çµæœæœƒé¡å¤–åˆ¤æ–·æ˜¯å¦ç‚ºæ¼¢åŒ–ç‰ˆã€‚
    """
    search_results = []
    
    queries_to_try = []

    # 1. å„ªå…ˆåŠ å…¥åŸå§‹çš„å®Œæ•´æ¨™é¡Œ (full_query)ï¼Œå› ç‚º WNACG çš„æœå°‹å¯èƒ½è™•ç†é€™äº›æ–¹æ‹¬è™Ÿ
    if full_query:
        queries_to_try.append(full_query)

    # 2. åŠ å…¥è§£æå‡ºçš„ä¸»æ¨™é¡Œ (å·²ç¶“éå¾¹åº•æ¸…ç†ï¼Œä¸å«æ–¹æ‹¬è™Ÿ) - ä½œç‚ºå¾Œå‚™æˆ–è¼”åŠ©
    if main_title and main_title not in queries_to_try:
        queries_to_try.append(main_title)

    # 3. åŠ å…¥è§£æå‡ºçš„å‰¯æ¨™é¡Œ (å·²ç¶“éå¾¹åº•æ¸…ç†ï¼Œä¸å«æ–¹æ‹¬è™Ÿ) - ä½œç‚ºå¾Œå‚™æˆ–è¼”åŠ©
    if subtitle and subtitle not in queries_to_try:
        queries_to_try.append(subtitle)
    
    # 4. åŠ å…¥é¡å¤–æ—¥æ–‡ä¸»æ¨™é¡Œ (å·²ç¶“éå¾¹åº•æ¸…ç†ï¼Œä¸å«æ–¹æ‹¬è™Ÿ) - ä½œç‚ºå¾Œå‚™æˆ–è¼”åŠ©
    if extra_japanese_main_title and extra_japanese_main_title not in queries_to_try:
        queries_to_try.append(extra_japanese_main_title)

    # 5. ç”Ÿæˆæ›´ç²¾ç°¡çš„æ¨™é¡Œè®Šé«” (å¾æœ€ä¹¾æ·¨çš„ä¸»æ¨™é¡Œé–‹å§‹)
    # å¦‚æœä¸»æ¨™é¡ŒåŒ…å«æ•¸å­—åºåˆ— (ä¾‹å¦‚ "Title 2" ä¸­çš„ "2")ï¼Œå˜—è©¦ç§»é™¤æ•¸å­—ä»¥æœå°‹ç³»åˆ—ç¬¬ä¸€éƒ¨
    if main_title and re.search(r'\s+\d+$', main_title): # åŒ¹é…ä»¥æ•¸å­—çµå°¾çš„æ¨™é¡Œ
        query_without_number = re.sub(r'\s+\d+$', '', main_title).strip()
        if query_without_number and query_without_number not in queries_to_try:
            queries_to_try.append(query_without_number)

    # å˜—è©¦ç”¨ ' - ' åˆ†å‰²æ¸…ç†å¾Œçš„æ ¸å¿ƒæ¨™é¡Œï¼Œå–ä¸»è¦éƒ¨åˆ† (å¦‚æœä¹‹å‰æ²’æœ‰ä»¥æ­¤åˆ†éš”)
    # é€™è£¡çš„ main_title å·²ç¶“æ˜¯æ¸…ç†å¾Œçš„äº†
    if ' - ' in main_title and main_title not in queries_to_try:
        parts = [p.strip() for p in main_title.split(' - ') if p.strip()]
        if len(parts) > 1:
            for p in parts: # éæ­·æ‰€æœ‰åˆ†å‰²éƒ¨åˆ†ï¼Œä½œç‚ºç¨ç«‹æŸ¥è©¢
                 if p and p not in queries_to_try: # é¿å…é‡è¤‡
                     queries_to_try.append(p)
    
    # å¦‚æœæ¨™é¡Œä»å¾ˆé•·ï¼Œå˜—è©¦åªå–å‰é¢ä¸€éƒ¨åˆ†è©
    if main_title and len(main_title.split()) > 5:
        short_query = ' '.join(main_title.split()[:5]).strip()
        if short_query and short_query not in queries_to_try:
            queries_to_try.append(short_query)
    
    # å»é™¤é‡è¤‡ä¸¦ç¢ºä¿éç©º
    queries_to_try = [q for q in list(dict.fromkeys(queries_to_try)) if q]
    
    logger.info(f"ç´³å£«æ¼«ç•«æœå°‹å˜—è©¦çš„æŸ¥è©¢è© (åŒ…æ‹¬ä¸»å‰¯æ¨™é¡Œè¡ç”Ÿ): {queries_to_try}")

    for q in queries_to_try:
        if not q: continue
        search_url = f"{WNACG_BASE_URL}search/?q={urllib.parse.quote(q)}"
        logger.info(f"ç´³å£«æ¼«ç•«æœå°‹ URL: {search_url}")
        
        try:
            response = requests.get(search_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            galleries = soup.select('li.gallary_item') 
            
            for gallery in galleries:
                title_tag = gallery.select_one('div.info div.title a')
                if title_tag:
                    # ä½¿ç”¨ BeautifulSoup è§£æ HTML ä¸¦ç²å–ç´”æ–‡å­—ï¼Œé€™æœƒè‡ªå‹•è™•ç† <em> æ¨™ç±¤
                    # æ³¨æ„: é€™è£¡çš„ title æ˜¯é¡¯ç¤ºç”¨çš„ï¼Œæœå°‹åŒ¹é…æ‡‰åŸºæ–¼ query
                    clean_title_soup = BeautifulSoup(str(title_tag), 'html.parser')
                    title = clean_title_soup.get_text(strip=True)

                    relative_url = title_tag.get('href')
                    if relative_url:
                        full_url = urllib.parse.urljoin(WNACG_BASE_URL, relative_url)
                        
                        # åœ¨é€™è£¡å‘¼å« get_wnacg_info å‡½æ•¸ä¾†ç²å–æ¼¢åŒ–ç‹€æ…‹
                        wnacg_info = get_wnacg_info(full_url)
                        is_translated = wnacg_info.get("is_translated", False)

                        search_results.append({
                            'title': title, 
                            'url': full_url,
                            'similarity': 0, # ç´³å£«æ¼«ç•«ä¸è¨ˆç®—ç›¸ä¼¼åº¦ï¼Œè¨­ç‚º0
                            'is_translated': is_translated # æ–°å¢æ¼¢åŒ–æ¨™èªŒ
                        })
            if search_results: 
                break # å¦‚æœæ‰¾åˆ°ä»»ä½•çµæœï¼Œå°±åœæ­¢æ›´å¤šå˜—è©¦ï¼Œé¿å…ä¸ç›¸é—œçµæœ

        except requests.exceptions.RequestException as e:
            logger.warning(f"ç´³å£«æ¼«ç•«æœå°‹ '{q}' å¤±æ•—: {e}")
        except Exception as e:
            logger.error(f"è§£æç´³å£«æ¼«ç•«æœå°‹çµæœ '{q}' æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
            
    unique_results = {}
    for res in search_results:
        unique_results[res['url']] = res
    
    return list(unique_results.values())[:5]


async def handle_e_ex_hentai_link(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """è™•ç† E-Hentai æˆ– ExHentai é€£çµï¼Œä¸¦æœå°‹ç›¸é—œçš„ Nhentai å’Œç´³å£«æ¼«ç•«ä½œå“ã€‚"""
    message = update.message 
    if not message or not message.text:
        return

    chat_id = message.chat.id
    message_id = message.message_id
    
    # ä½¿ç”¨å…¨åŸŸå®šç¾©çš„ e_ex_hentai_regex ä¾†æå–è¨Šæ¯ä¸­çš„å¯¦éš›é€£çµ
    match = e_ex_hentai_regex.search(message.text)
    if not match:
        # ç†è«–ä¸Šé€™å€‹åˆ†æ”¯ä¸æ‡‰è©²è¢«è§¸ç™¼ï¼Œå› ç‚º MessageHandler çš„ filter å·²ç¶“åŒ¹é…äº†
        logger.warning(f"è™•ç†å™¨è§¸ç™¼ä½†æœªåœ¨è¨Šæ¯ä¸­æ‰¾åˆ° E-Hentai é€£çµ: {message.text}")
        return 
    
    link = match.group(0) # æå–åŒ¹é…åˆ°çš„å®Œæ•´é€£çµå­—ä¸²
    
    # æª¢æŸ¥é€™å€‹è¨Šæ¯çš„ç•¶å‰é€£çµå…§å®¹æ˜¯å¦å’Œç·©å­˜ä¸­å·²è™•ç†çš„å…§å®¹ç›¸åŒ
    if PROCESSED_LINKS_CACHE.get((chat_id, message_id)) == link:
        logger.info(f"è¨Šæ¯ {message_id} å·²è™•ç†éé€£çµ {link}ï¼Œè·³éé‡æ–°è™•ç†ã€‚")
        return

    processing_message = await context.bot.send_message(
        chat_id=chat_id,
        text="æ­£åœ¨åˆ†æé€£çµä¸¦æœå°‹ç›¸é—œä½œå“ï¼Œè«‹ç¨å€™...",
        reply_to_message_id=message_id 
    )

    try:
        # Step 1: å¾ E-Hentai/ExHentai é€£çµç²å–ç•«å»Šè³‡è¨Š
        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=processing_message.message_id,
            text="[ğŸŸ©â¬œâ¬œ] æ­£åœ¨å¾ E-Hentai/ExHentai ç²å–ç•«å»Šè³‡è¨Š..."
        )
        # full_gallery_title æ˜¯åŸå§‹çš„ã€æœªç¶“ clean_title_string è™•ç†çš„æ¨™é¡Œ
        e_ex_hentai_title, e_ex_hentai_tags, main_title_parsed, subtitle_parsed, extra_japanese_main_title = get_e_ex_hentai_info(link)

        if e_ex_hentai_title == "ç²å–å¤±æ•—" or not e_ex_hentai_title:
            await context.bot.edit_message_text(
                chat_id=chat_id,
                message_id=processing_message.message_id,
                text="âŒ ç„¡æ³•å¾ E-Hentai/ExHentai é€£çµç²å–ç•«å»Šè³‡è¨Šï¼Œè«‹æª¢æŸ¥é€£çµæ˜¯å¦æœ‰æ•ˆæˆ–æ¬Šé™æ˜¯å¦è¶³å¤ ã€‚"
            )
            PROCESSED_LINKS_CACHE[(chat_id, message_id)] = link
            return

        # Step 2: æœå°‹ Nhentai 
        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=processing_message.message_id,
            text=f"[ğŸŸ©ğŸŸ©â¬œ] æ­£åœ¨ Nhentai æœå°‹ `{e_ex_hentai_title}` ç›¸é—œä½œå“...",
            parse_mode='Markdown'
        )
        # å°‡æ‰€æœ‰ç›¸é—œæ¨™é¡Œå‚³å…¥ Nhentai æœå°‹å‡½æ•¸
        nhentai_results = search_nhentai(e_ex_hentai_title, main_title_parsed, subtitle_parsed, extra_japanese_main_title, e_ex_hentai_tags)

        # Step 3: æœå°‹ç´³å£«æ¼«ç•« (wnacg.com)
        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=processing_message.message_id,
            text=f"[ğŸŸ©ğŸŸ©ğŸŸ©] æ­£åœ¨ç´³å£«æ¼«ç•«æœå°‹ `{e_ex_hentai_title}` ç›¸é—œä½œå“...", # é€™è£¡é¡¯ç¤ºåŸå§‹æ¨™é¡Œï¼Œå› ç‚ºæœƒå„ªå…ˆä½¿ç”¨å®ƒä¾†æœå°‹
            parse_mode='Markdown'
        )
        wnacg_results = search_wnacg_by_title(e_ex_hentai_title, main_title_parsed, subtitle_parsed, extra_japanese_main_title)

        # Step 4: æ§‹å»ºå›è¦†æ¶ˆæ¯
        response_text = "" 
        
        if nhentai_results:
            response_text += "âœ¨ *Nhentai ç›¸é—œä½œå“ (ç›¸ä¼¼åº¦):*\n"
            for i, result in enumerate(nhentai_results):
                stars = "â­" * int(result['similarity'] // 20)
                response_text += f"{i+1}. {result['url']} ({stars} {result['similarity']:.1f}%)\n" 
        else:
            response_text += "âŒ *Nhentai ç›¸é—œä½œå“:* æœªæ‰¾åˆ°åŒ¹é…çµæœã€‚\n"

        response_text += "\n"

        if wnacg_results:
            response_text += "ğŸ“š *ç´³å£«æ¼«ç•«ç›¸é—œä½œå“ (æ¨™é¡ŒåŒ¹é…):*\n"
            for i, result in enumerate(wnacg_results):
                translated_tag = " (æ¼¢åŒ–ç‰ˆ)" if result.get('is_translated', False) else ""
                response_text += f"{i+1}. {result['url']}{translated_tag}\n" 
        else:
            response_text += "âŒ *ç´³å£«æ¼«ç•«ç›¸é—œä½œå“:* æœªæ‰¾åˆ°åŒ¹é…çµæœã€‚\n"

        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=processing_message.message_id,
            text=response_text,
            parse_mode='Markdown',
            disable_web_page_preview=True
        )
        
        PROCESSED_LINKS_CACHE[(chat_id, message_id)] = link

    except Exception as e:
        logger.error(f"è™•ç† E-Hentai/ExHentai é€£çµæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
        await context.bot.edit_message_text(
            chat_id=chat_id,
            message_id=processing_message.message_id,
            text=f"å“å‘€ï¼è™•ç†æ‚¨çš„é€£çµæ™‚ç™¼ç”Ÿäº†éŒ¯èª¤ï¼š`{str(e)}`",
            parse_mode='Markdown'
        )
        PROCESSED_LINKS_CACHE[(chat_id, message_id)] = link


# é‡å¯«æ­¤å‡½æ•¸ï¼Œæ¡ç”¨æ›´æ˜ç¢ºçš„æ›¿æ›é‚è¼¯
async def handle_x_link(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """è™•ç† X.com (Twitter) é€£çµï¼Œè½‰æ›ç‚º VxTwitter é€£çµã€‚"""
    message = update.message
    if not message or not message.text:
        return

    chat_id = message.chat.id
    message_id = message.message_id

    # ä½¿ç”¨å…¨åŸŸå®šç¾©çš„ x_link_regex ä¾†æå–è¨Šæ¯ä¸­çš„å¯¦éš›é€£çµ
    match = x_link_regex.search(message.text)
    if not match:
        logger.warning(f"è™•ç†å™¨è§¸ç™¼ä½†æœªåœ¨è¨Šæ¯ä¸­æ‰¾åˆ° X.com æˆ– Twitter.com é€£çµ: {message.text}")
        return
        
    original_link = match.group(0).strip() # æå–åŒ¹é…åˆ°çš„å®Œæ•´é€£çµå­—ä¸²ä¸¦å»é™¤å‰å¾Œç©ºæ ¼

    if PROCESSED_LINKS_CACHE.get((chat_id, message_id)) == original_link:
        logger.info(f"è¨Šæ¯ {message_id} å·²è™•ç†é X.com é€£çµ {original_link}ï¼Œè·³éã€‚")
        return

    try:
        logger.info(f"Original X.com/Twitter.com link received: {original_link}")

        new_url = ""
        # æª¢æŸ¥é€£çµæ˜¯å¦ä»¥ x.com é–‹é ­
        if "x.com" in original_link.lower():
            new_url = original_link.replace("x.com", "vxtwitter.com")
        # æª¢æŸ¥é€£çµæ˜¯å¦ä»¥ twitter.com é–‹é ­
        elif "twitter.com" in original_link.lower():
            new_url = original_link.replace("twitter.com", "vxtwitter.com")
        else:
            # ç†è«–ä¸Šï¼Œç”±æ–¼ regex çš„éæ¿¾ï¼Œé€™å€‹åˆ†æ”¯ä¸æ‡‰è©²è¢«è§¸ç™¼
            logger.warning(f"æ”¶åˆ°éé æœŸçš„ X/Twitter é€£çµæ ¼å¼ï¼Œæœªèƒ½è½‰æ›: {original_link}")
            await update.message.reply_text(
                f"ç„¡æ³•è½‰æ›æ­¤é€£çµï¼š`{original_link}`ã€‚è«‹ç¢ºä¿å®ƒæ˜¯æ¨™æº–çš„ X.com æˆ– Twitter.com é€£çµã€‚",
                reply_to_message_id=message_id,
                parse_mode='Markdown'
            )
            return

        # ç¢ºä¿åªæœ‰åœ¨æˆåŠŸç”Ÿæˆ new_url å¾Œæ‰é€²è¡Œæ—¥èªŒè¨˜éŒ„å’Œå›è¦†
        if new_url:
            logger.info(f"Converted VxTwitter link: {new_url}")

            await update.message.reply_text(
                f"å·²è½‰æ›ç‚ºæ›´å¥½çš„é è¦½é€£çµï¼š\n{new_url}",
                reply_to_message_id=message_id,
                disable_web_page_preview=False
            )
            PROCESSED_LINKS_CACHE[(chat_id, message_id)] = original_link
            logger.info(f"X.com/Twitter.com é€£çµ {original_link} å·²æˆåŠŸè½‰æ›ä¸¦å›å‚³é€£çµ {new_url} çµ¦ {message.from_user.id if message.from_user else 'æœªçŸ¥ç”¨æˆ¶'}")
    except Exception as e:
        logger.error(f"è™•ç† X.com/Twitter.com é€£çµæ™‚ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}\n{traceback.format_exc()}")
        await update.message.reply_text(
            f"å“å‘€ï¼è™•ç†æ‚¨çš„ X.com/Twitter.com é€£çµæ™‚ç™¼ç”Ÿäº†éŒ¯èª¤ï¼š`{str(e)}`",
            reply_to_message_id=message_id,
            parse_mode='Markdown'
        )


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """è™•ç†æ©Ÿå™¨äººé‹ä½œä¸­ç™¼ç”Ÿçš„æ‰€æœ‰éŒ¯èª¤ã€‚"""
    logger.error(f"æ›´æ–° {update} å°è‡´éŒ¯èª¤ {context.error}")
    if update and update.effective_message:
        await update.effective_message.reply_text(
            "æŠ±æ­‰ï¼Œæ©Ÿå™¨äººç™¼ç”Ÿäº†ä¸€äº›å…§éƒ¨éŒ¯èª¤ã€‚è«‹ç¨å¾Œå†è©¦ã€‚",
            reply_to_message_id=update.effective_message.message_id
        )

def main() -> None:
    """ä¸»å‡½æ•¸ï¼Œå•Ÿå‹•æ©Ÿå™¨äººã€‚"""
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    application.add_handler(CommandHandler("start", start_command))
    
    # è™•ç†æ–°è¨Šæ¯ï¼ˆç¾åœ¨æœƒæ™ºæ…§æå–é€£çµï¼‰
    application.add_handler(MessageHandler(filters.TEXT & filters.Regex(e_ex_hentai_regex), handle_e_ex_hentai_link))
    # Crucially, ensure the regex for x_link_regex *only* matches x.com or twitter.com
    # and not an already converted vxtwitter.com link, which would cause double conversion.
    # The current regex `r'https?://(?:x|twitter)\.com/\S+'` already handles this correctly,
    # as it doesn't match 'vxtwitter.com'.
    application.add_handler(MessageHandler(filters.TEXT & filters.Regex(x_link_regex), handle_x_link))

    application.add_error_handler(error_handler)

    logger.info("æ©Ÿå™¨äººå·²å•Ÿå‹•ï¼Œé–‹å§‹ç›£è½è¨Šæ¯...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == '__main__':
    main()